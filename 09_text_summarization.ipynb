{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inside-pleasure",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "Using abstraction or extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-smoke",
   "metadata": {},
   "source": [
    "**Extractive Method**\n",
    "\n",
    "Three main steps:\n",
    "1. Create an intermediate representation of the text.\n",
    "2. Score the sentences/phrases based on the choosen representation.\n",
    "3. Rank and choose sentences to create a summary of the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-preservation",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "humanitarian-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reprlib\n",
    "\n",
    "r = reprlib.Repr()\n",
    "r.maxstring = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grateful-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ideal-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def download_article(url):\n",
    "    # Check if article is already there.\n",
    "    filename = url.split(\"/\")[-1] + \".html\"\n",
    "    if not os.path.isfile(filename):\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"w+\") as f:\n",
    "            f.write(r.text)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def parse_article(article_file):\n",
    "    with open(article_file, \"r\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    r = {}\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    r[\"url\"] = soup.find(\"link\", {\"rel\": \"canonical\"})[\"href\"]\n",
    "    r[\"headline\"] = soup.h1.text\n",
    "    children = list(soup.select_one(\"article\").children)[0]\n",
    "    paragraphs = [child.text for child in children if child.name != \"div\"]\n",
    "    r[\"text\"] = \"\\n\".join(paragraphs)\n",
    "    r[\"authors\"] = [\n",
    "        a.text for a in soup.select(\".ArticleBody-byline-container-3H6dy a\")\n",
    "    ]\n",
    "    r[\"time\"] = soup.find(\"meta\", {\"property\": \"og:article:published_time\"})[\"content\"]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "juvenile-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article published on '2018-03-15T11:37:01Z'\n",
      "\"LONDON/SAN FRANCISCO (Reuters) - U.S. President Donald Trump has blocked microchip maker Broadcom Ltd's AVGO.O $117 billion takeover of rival Qualcomm QCOM.O amid concerns that it would give China the upper hand in the next generation of mobile communications, or 5G.\\nA 5G sign is seen at the Mobile World Congress in Barcelona, Spain February 28, 2018. REUTERS/Yves Herman\\nBelow are some facts ... and telecommunications gear makers will have to pay it licensing fees. It dominated standards setting in 3G and 4G wireless and looks set to top the list of patent holders heading into the 5G cycle.\\nHuawei, Nokia, Ericsson and others are also vying to amass 5G patents, which has helped spur complex cross-licensing agreements like the deal struck late last year Nokia and Huawei around handsets.\"\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://www.reuters.com/article/us-qualcomm-m-a-broadcom-5g/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN\"\n",
    "\n",
    "article_name1 = download_article(url1)\n",
    "article1 = parse_article(article_name1)\n",
    "print(\"Article published on\", r.repr(article1[\"time\"]))\n",
    "print(r.repr(article1[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-expense",
   "metadata": {},
   "source": [
    "### Blueprint: Summarizing Text using Topic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-world",
   "metadata": {},
   "source": [
    "**Identifying Important Words with TF-IDF values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "manual-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tfidf_summary(article, num_summary_sentences=3):\n",
    "    sentences = tokenize.sent_tokenize(article)\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    words_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Sort the sentences in descending order by the sum of TF-IDF values.\n",
    "    sent_sum = words_tfidf.sum(axis=1)  # One column.\n",
    "    important_sent = np.argsort(sent_sum, axis=0)[::-1]\n",
    "\n",
    "    result = []\n",
    "    # Return three most important sentences in the order they appear in the article.\n",
    "    for i in range(len(sentences)):\n",
    "        if i in important_sent[:num_summary_sentences]:\n",
    "            result.append(sentences[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "covered-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON/SAN FRANCISCO (Reuters) - U.S. President Donald Trump has blocked microchip maker Broadcom Ltd's AVGO.O $117 billion takeover of rival Qualcomm QCOM.O amid concerns that it would give China the upper hand in the next generation of mobile communications, or 5G.\n",
      "\n",
      "5G networks, now in the final testing stage, will rely on denser arrays of small antennas and the cloud to offer data speeds up to 50 or 100 times faster than current 4G networks and serve as critical infrastructure for a range of industries.\n",
      "\n",
      "Most other baseband chips come from Asia: MediaTek 2454.TW of Taiwan holds about one quarter of the market, while Samsung Electronics 005930.KS and Huawei [HWT.UL] - two big smartphone makers - develop chips for their own devices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in tfidf_summary(article1[\"text\"]):\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-lobby",
   "metadata": {},
   "source": [
    "## LSA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "blind-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "transparent-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON/SAN FRANCISCO (Reuters) - U.S. President Donald Trump has blocked microchip maker Broadcom Ltd's AVGO.O $117 billion takeover of rival Qualcomm QCOM.O amid concerns that it would give China the upper hand in the next generation of mobile communications, or 5G.\n",
      "\n",
      "Moving to new networks promises to enable new mobile services and even whole new business models, but could pose challenges for countries and industries unprepared to invest in the transition.\n",
      "\n",
      "The concern is that a takeover by Singapore-based Broadcom could see the firm cut research and development spending by Qualcomm or hive off strategically important parts of the company to other buyers, including in China, U.S. officials and analysts have said.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "\n",
    "\n",
    "def lsa_summary(article, num_summary_sentences=3):\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(LANGUAGE))\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    return [\n",
    "        str(sentence) for sentence in summarizer(parser.document, num_summary_sentences)\n",
    "    ]\n",
    "\n",
    "\n",
    "for sentence in lsa_summary(article1[\"text\"]):\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-causing",
   "metadata": {},
   "source": [
    "### Blueprint: Summarizing Text using an Indicator Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "advisory-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring Qualcomm would represent the jewel in the crown of Broadcom’s portfolio of communications chips, which supply wi-fi, power management, video and other features in smartphones alongside Qualcomm’s core baseband chips - radio modems that wirelessly connect phones to networks.\n",
      "\n",
      "Qualcomm QCOM.O is the dominant player in smartphone communications chips, making half of all core baseband radio chips in smartphones.\n",
      "\n",
      "The standards are set by a global body to ensure all phones work across different mobile networks, and whoever’s essential patents end up making it into the standard stands to reap huge royalty licensing revenue streams.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "\n",
    "def textrank_summary(article, num_summary_sentences=3):\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(LANGUAGE))\n",
    "    summarizer = TextRankSummarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    return [\n",
    "        str(sentence) for sentence in summarizer(parser.document, num_summary_sentences)\n",
    "    ]\n",
    "\n",
    "\n",
    "for sentence in textrank_summary(article1[\"text\"]):\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-small",
   "metadata": {},
   "source": [
    "### Measuring the performance of Text Summarization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "outdoor-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "informative-approval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.05, recall=0.5555555555555556, fmeasure=0.09174311926605504),\n",
       " 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
       " 'rougeL': Score(precision=0.03, recall=0.3333333333333333, fmeasure=0.055045871559633024)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "num_summary_sentences = 3\n",
    "gold_standard = article1[\"headline\"]\n",
    "summary = \"\".join(textrank_summary(article1[\"text\"], num_summary_sentences))\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "muslim-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.03389830508474576, recall=0.4444444444444444, fmeasure=0.06299212598425197),\n",
       " 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
       " 'rougeL': Score(precision=0.025423728813559324, recall=0.3333333333333333, fmeasure=0.04724409448818898)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = \"\".join(lsa_summary(article1[\"text\"], num_summary_sentences))\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(gold_standard, summary)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-theme",
   "metadata": {},
   "source": [
    "### Blueprint: Summarizing Text using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-chassis",
   "metadata": {},
   "source": [
    "**Step 1: Creating Target Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "worse-score",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>850</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Filename</th>\n",
       "      <td>60763_5_3122150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThreadID</th>\n",
       "      <td>60763_5_3122150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>which attractions need to be pre booked?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userID</th>\n",
       "      <td>musicqueenLon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>29 September 2009, 1:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postNum</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Hi    I am coming to NY in Oct! So excited&amp;quo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>A woman was planning to travel NYC in October ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        850\n",
       "Filename                                    60763_5_3122150\n",
       "ThreadID                                    60763_5_3122150\n",
       "Title              which attractions need to be pre booked?\n",
       "userID                                     musicqueenLon...\n",
       "Date                                29 September 2009, 1:41\n",
       "postNum                                                   1\n",
       "text      Hi    I am coming to NY in Oct! So excited&quo...\n",
       "summary   A woman was planning to travel NYC in October ..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/travel_threads.csv.gz\", sep=\"|\", dtype={\"ThreadID\": \"object\"})\n",
    "df[df[\"ThreadID\"] == \"60763_5_3122150\"].head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "helpful-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-using the blueprint from Chapter 4 but adapting to add additional steps specific to this dataset\n",
    "import re  # ##\n",
    "\n",
    "import spacy  # ##\n",
    "import textacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex\n",
    "from textacy.preprocessing.replace import urls as replace_urls\n",
    "\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "\n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [\n",
    "        pattern for pattern in nlp.Defaults.prefixes if pattern not in [\"-\", \"_\", \"#\"]\n",
    "    ]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes if pattern not in [\"_\"]]\n",
    "    infixes = [\n",
    "        pattern for pattern in nlp.Defaults.infixes if not re.search(pattern, \"xx-xx\")\n",
    "    ]\n",
    "\n",
    "    return Tokenizer(\n",
    "        vocab=nlp.vocab,\n",
    "        rules=nlp.Defaults.tokenizer_exceptions,\n",
    "        prefix_search=compile_prefix_regex(prefixes).search,\n",
    "        suffix_search=compile_suffix_regex(suffixes).search,\n",
    "        infix_finditer=compile_infix_regex(infixes).finditer,\n",
    "        token_match=nlp.Defaults.token_match,\n",
    "    )\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "\n",
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "\n",
    "def extract_noun_chunks(doc, include_pos=[\"NOUN\"], sep=\"_\"):\n",
    "    chunks = []\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        chunk = [token.lemma_ for token in noun_chunk if token.pos_ in include_pos]\n",
    "        if len(chunk) >= 2:\n",
    "            chunks.append(sep.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_entities(doc, include_types=None, sep=\"_\"):\n",
    "\n",
    "    ents = textacy.extract.entities(\n",
    "        doc,\n",
    "        include_types=include_types,\n",
    "        exclude_types=None,\n",
    "        drop_determiners=True,\n",
    "        min_freq=1,\n",
    "    )\n",
    "\n",
    "    return [re.sub(\"\\s+\", sep, e.lemma_) + \"/\" + e.label_ for e in ents]\n",
    "\n",
    "\n",
    "def spacy_clean(text):\n",
    "    # Replace URLs\n",
    "    text = replace_urls(text)\n",
    "\n",
    "    # Replace semi-colons (relevant in Java code ending)\n",
    "    text = text.replace(\";\", \"\")\n",
    "\n",
    "    # Replace character tabs (present as literal in description field)\n",
    "    text = text.replace(\"\\t\", \"\")\n",
    "\n",
    "    # Find and remove any stack traces - doesn't fix all code fragments but removes many exceptions\n",
    "    start_loc = text.find(\"Stack trace:\")\n",
    "    text = text[:start_loc]\n",
    "\n",
    "    # Remove Hex Code\n",
    "    text = re.sub(r\"(\\w+)0x\\w+\", \"\", text)\n",
    "\n",
    "    # Initialize Spacy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # From Blueprint function\n",
    "    lemmas = extract_lemmas(\n",
    "        doc,\n",
    "        exclude_pos=[\"PART\", \"PUNCT\", \"DET\", \"PRON\", \"SYM\", \"SPACE\", \"NUM\"],\n",
    "        filter_stops=True,\n",
    "        filter_nums=True,\n",
    "        filter_punct=True,\n",
    "    )\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "regulated-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "earned-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(clean)\n",
    "df[\"lemmas\"] = df[\"text\"].apply(spacy_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "upper-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads for Training 559\n",
      "Number of threads for Testing 140\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_split, test_split = next(gss.split(df, groups=df[\"ThreadID\"]))\n",
    "train_df = df.iloc[train_split]\n",
    "test_df = df.iloc[test_split]\n",
    "\n",
    "print(\"Number of threads for Training\", train_df[\"ThreadID\"].nunique())\n",
    "print(\"Number of threads for Testing\", test_df[\"ThreadID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "harmful-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "diverse-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/1569446990.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"similarity\"] = train_df.apply(\n",
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/1569446990.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"rank\"] = train_df.groupby(\"ThreadID\")[\"similarity\"].rank(\n",
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/1569446990.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"summary_post\"] = train_df.groupby(\"ThreadID\")[\"rank\"].apply(top_n)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Hi I am coming to NY in Oct! So excited\" Have ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>I wouldnt bother doing the ESB if I was you TO...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>The Statue of Liberty, if you plan on going to...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  summary_post\n",
       "850  Hi I am coming to NY in Oct! So excited\" Have ...          True\n",
       "851  I wouldnt bother doing the ESB if I was you TO...         False\n",
       "852  The Statue of Liberty, if you plan on going to...          True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textdistance\n",
    "\n",
    "compression_factor = 0.3\n",
    "\n",
    "train_df[\"similarity\"] = train_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1\n",
    ")\n",
    "train_df[\"rank\"] = train_df.groupby(\"ThreadID\")[\"similarity\"].rank(\n",
    "    \"max\", ascending=False\n",
    ")\n",
    "\n",
    "top_n = lambda x: x <= np.ceil(compression_factor * x.max())\n",
    "\n",
    "train_df[\"summary_post\"] = train_df.groupby(\"ThreadID\")[\"rank\"].apply(top_n)\n",
    "train_df[[\"text\", \"summary_post\"]][train_df[\"ThreadID\"] == \"60763_5_3122150\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ignored-liberty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "train_df.loc[train_df[\"text\"].str.len() <= 20, \"summary_post\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-cleaner",
   "metadata": {},
   "source": [
    "**Step 2: Adding Features to Assist Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "proprietary-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"title_similarity\"] = df.apply(\n",
    "        lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1\n",
    "    )\n",
    "    df[\"text_length\"] = df[\"text\"].str.len()\n",
    "    df[\"combined\"] = [\" \".join(map(str, l)) for l in df[\"lemmas\"] if l != \"\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def vectorizer(df, tfidf):\n",
    "    feature_cols = [\"title_similarity\", \"text_length\", \"postNum\"]\n",
    "    tfidf_result = tfidf.transform(df[\"combined\"]).toarray()\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names())\n",
    "    tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "    tfidf_df.index = df.index\n",
    "    df_tf = pd.concat([df[feature_cols], tfidf_df], axis=1)\n",
    "    return df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "functional-demand",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/1340852196.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"title_similarity\"] = train_df.apply(\n"
     ]
    }
   ],
   "source": [
    "train_df[\"title_similarity\"] = train_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "serial-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/1956652621.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text_length'] = train_df['text'].str.len()\n"
     ]
    }
   ],
   "source": [
    "# Adding post length as a feature.\n",
    "train_df[\"text_length\"] = train_df[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "played-still",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/74_ct3hx33d878n626w1wxyc0000gn/T/ipykernel_35346/283815228.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"combined\"] = [\" \".join(map(str, l)) for l in train_df[\"lemmas\"] if l != \"\"]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\"title_similarity\", \"text_length\", \"postNum\"]\n",
    "train_df[\"combined\"] = [\" \".join(map(str, l)) for l in train_df[\"lemmas\"] if l != \"\"]\n",
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
    "tfidf_result = tfidf.fit_transform(train_df[\"combined\"]).toarray()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = train_df.index\n",
    "train_df_tf = pd.concat([train_df[feature_cols], tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-floating",
   "metadata": {},
   "source": [
    "**Step 3: Build a Machine Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "promotional-spring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model1 = RandomForestClassifier()\n",
    "model1.fit(train_df_tf, train_df[\"summary_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "addressed-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROUGE-1 Score for test threads 0.3463420408926299\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate rouge score for each thread.\n",
    "def calculate_rouge_score(x, column_name):\n",
    "    # Get the original summary - only first value since they are repeated.\n",
    "    ref_summary = x[\"summary\"].values[0]\n",
    "\n",
    "    # Join all posts that have been predicted as summary.\n",
    "    predicted_summary = \"\".join(x[\"text\"][x[column_name]])\n",
    "\n",
    "    # Return the rouge score for each ThreadID.\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\n",
    "    scores = scorer.score(ref_summary, predicted_summary)\n",
    "    return scores[\"rouge1\"].fmeasure\n",
    "\n",
    "\n",
    "test_df = preprocess(test_df)\n",
    "test_df_tf = vectorizer(test_df, tfidf)\n",
    "\n",
    "test_df[\"predicted_summary_post\"] = model1.predict(test_df_tf)\n",
    "print(\n",
    "    \"Mean ROUGE-1 Score for test threads\",\n",
    "    test_df.groupby(\"ThreadID\")[[\"summary\", \"text\", \"predicted_summary_post\"]]\n",
    "    .apply(calculate_rouge_score, column_name=\"predicted_summary_post\")\n",
    "    .mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "grand-spirit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60763_5_3144153'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(2)\n",
    "thread_id = random.sample(test_df[\"ThreadID\"].unique().tolist(), 1)[0]\n",
    "thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "familiar-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of posts 11\n",
      "Number of summary posts 1\n",
      "Title:  The Information Deficit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postNum</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "      <td>From today's TImes: Afzal Hossain is the New Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     postNum                                               text\n",
       "397        1  From today's TImes: Afzal Hossain is the New Y..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = test_df[test_df[\"ThreadID\"] == thread_id]\n",
    "print(\"Total number of posts\", example_df[\"postNum\"].max())\n",
    "print(\n",
    "    \"Number of summary posts\",\n",
    "    example_df[example_df[\"predicted_summary_post\"]].count().values[0],\n",
    ")\n",
    "print(\"Title: \", example_df[\"Title\"].values[0])\n",
    "example_df[[\"postNum\", \"text\"]][example_df[\"predicted_summary_post\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "geographic-sacramento",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>userID</th>\n",
       "      <th>Date</th>\n",
       "      <th>postNum</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>text_length</th>\n",
       "      <th>combined</th>\n",
       "      <th>predicted_summary_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>J-A-W-P</td>\n",
       "      <td>09 October 2009, 20:55</td>\n",
       "      <td>1</td>\n",
       "      <td>From today's TImes: Afzal Hossain is the New Y...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[today, TImes, Afzal, Hossain, New, York, City...</td>\n",
       "      <td>0.520931</td>\n",
       "      <td>1432</td>\n",
       "      <td>today TImes Afzal Hossain New York City subway...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>Outoftheinkwe...</td>\n",
       "      <td>09 October 2009, 21:22</td>\n",
       "      <td>2</td>\n",
       "      <td>\"For a fee, Ugarte.\"</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[fee, Ugarte]</td>\n",
       "      <td>0.465620</td>\n",
       "      <td>20</td>\n",
       "      <td>fee Ugarte</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>Outoftheinkwe...</td>\n",
       "      <td>09 October 2009, 21:26</td>\n",
       "      <td>3</td>\n",
       "      <td>Oops! I think I misquoted my quote... should b...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[oops, think, misquote, quote, price, Ugarte]</td>\n",
       "      <td>0.436232</td>\n",
       "      <td>70</td>\n",
       "      <td>oops think misquote quote price Ugarte</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>livetotravel</td>\n",
       "      <td>09 October 2009, 21:45</td>\n",
       "      <td>4</td>\n",
       "      <td>there's a couple of great iPhone apps CityTran...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[couple, great, iPhone, app, CityTransit, map,...</td>\n",
       "      <td>0.527734</td>\n",
       "      <td>176</td>\n",
       "      <td>couple great iPhone app CityTransit map line s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>queensbouleva...</td>\n",
       "      <td>10 October 2009, 1:32</td>\n",
       "      <td>5</td>\n",
       "      <td>These young ladies were already ahead of the c...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[young, lady, ahead, curve, speak, April, adn,...</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>300</td>\n",
       "      <td>young lady ahead curve speak April adn MTA law...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>queensbouleva...</td>\n",
       "      <td>10 October 2009, 1:35</td>\n",
       "      <td>6</td>\n",
       "      <td>More on the Subway Service Specialists, from t...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[Subway, Service, Specialists, too-close-too-h...</td>\n",
       "      <td>0.497337</td>\n",
       "      <td>117</td>\n",
       "      <td>Subway Service Specialists too-close-too-home ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>Lotuspath</td>\n",
       "      <td>10 October 2009, 1:38</td>\n",
       "      <td>7</td>\n",
       "      <td>Hey .... if we're all gonna do this 'thing' .....</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[hey, gon, thing, willing, expect, actually, a...</td>\n",
       "      <td>0.505086</td>\n",
       "      <td>209</td>\n",
       "      <td>hey gon thing willing expect actually answer t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>Crans</td>\n",
       "      <td>10 October 2009, 1:44</td>\n",
       "      <td>8</td>\n",
       "      <td>LOL, QB! I don't even like to sit on those woo...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[lol, QB, like, sit, wooden, platform, bench, ...</td>\n",
       "      <td>0.536962</td>\n",
       "      <td>202</td>\n",
       "      <td>lol QB like sit wooden platform bench jean lon...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>BrooklynMel</td>\n",
       "      <td>10 October 2009, 1:53</td>\n",
       "      <td>9</td>\n",
       "      <td>QB, that's awesome. I'll bet that the no-fun M...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[QB, awesome, bet, no-fun, MTA, shut, love, sn...</td>\n",
       "      <td>0.465761</td>\n",
       "      <td>115</td>\n",
       "      <td>QB awesome bet no-fun MTA shut love snappy uni...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>Hankshanker</td>\n",
       "      <td>10 October 2009, 7:21</td>\n",
       "      <td>10</td>\n",
       "      <td>Nothing personal, but I think 1,000 percent ri...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[personal, think, percent, ridership, system, ...</td>\n",
       "      <td>0.522549</td>\n",
       "      <td>335</td>\n",
       "      <td>personal think percent ridership system person...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>60763_5_3144153</td>\n",
       "      <td>The Information Deficit</td>\n",
       "      <td>livetotravel</td>\n",
       "      <td>10 October 2009, 21:47</td>\n",
       "      <td>11</td>\n",
       "      <td>Where are they today I'm marooned in Brooklyn ...</td>\n",
       "      <td>A person initiated discussins about new york c...</td>\n",
       "      <td>[today, marooned, Brooklyn, train, run, want, ...</td>\n",
       "      <td>0.522266</td>\n",
       "      <td>104</td>\n",
       "      <td>today marooned Brooklyn train run want help</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Filename         ThreadID                    Title  \\\n",
       "397  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "398  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "399  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "400  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "401  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "402  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "403  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "404  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "405  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "406  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "407  60763_5_3144153  60763_5_3144153  The Information Deficit   \n",
       "\n",
       "               userID                    Date  postNum  \\\n",
       "397           J-A-W-P  09 October 2009, 20:55        1   \n",
       "398  Outoftheinkwe...  09 October 2009, 21:22        2   \n",
       "399  Outoftheinkwe...  09 October 2009, 21:26        3   \n",
       "400      livetotravel  09 October 2009, 21:45        4   \n",
       "401  queensbouleva...   10 October 2009, 1:32        5   \n",
       "402  queensbouleva...   10 October 2009, 1:35        6   \n",
       "403         Lotuspath   10 October 2009, 1:38        7   \n",
       "404             Crans   10 October 2009, 1:44        8   \n",
       "405       BrooklynMel   10 October 2009, 1:53        9   \n",
       "406       Hankshanker   10 October 2009, 7:21       10   \n",
       "407      livetotravel  10 October 2009, 21:47       11   \n",
       "\n",
       "                                                  text  \\\n",
       "397  From today's TImes: Afzal Hossain is the New Y...   \n",
       "398                               \"For a fee, Ugarte.\"   \n",
       "399  Oops! I think I misquoted my quote... should b...   \n",
       "400  there's a couple of great iPhone apps CityTran...   \n",
       "401  These young ladies were already ahead of the c...   \n",
       "402  More on the Subway Service Specialists, from t...   \n",
       "403  Hey .... if we're all gonna do this 'thing' .....   \n",
       "404  LOL, QB! I don't even like to sit on those woo...   \n",
       "405  QB, that's awesome. I'll bet that the no-fun M...   \n",
       "406  Nothing personal, but I think 1,000 percent ri...   \n",
       "407  Where are they today I'm marooned in Brooklyn ...   \n",
       "\n",
       "                                               summary  \\\n",
       "397  A person initiated discussins about new york c...   \n",
       "398  A person initiated discussins about new york c...   \n",
       "399  A person initiated discussins about new york c...   \n",
       "400  A person initiated discussins about new york c...   \n",
       "401  A person initiated discussins about new york c...   \n",
       "402  A person initiated discussins about new york c...   \n",
       "403  A person initiated discussins about new york c...   \n",
       "404  A person initiated discussins about new york c...   \n",
       "405  A person initiated discussins about new york c...   \n",
       "406  A person initiated discussins about new york c...   \n",
       "407  A person initiated discussins about new york c...   \n",
       "\n",
       "                                                lemmas  title_similarity  \\\n",
       "397  [today, TImes, Afzal, Hossain, New, York, City...          0.520931   \n",
       "398                                      [fee, Ugarte]          0.465620   \n",
       "399      [oops, think, misquote, quote, price, Ugarte]          0.436232   \n",
       "400  [couple, great, iPhone, app, CityTransit, map,...          0.527734   \n",
       "401  [young, lady, ahead, curve, speak, April, adn,...          0.518157   \n",
       "402  [Subway, Service, Specialists, too-close-too-h...          0.497337   \n",
       "403  [hey, gon, thing, willing, expect, actually, a...          0.505086   \n",
       "404  [lol, QB, like, sit, wooden, platform, bench, ...          0.536962   \n",
       "405  [QB, awesome, bet, no-fun, MTA, shut, love, sn...          0.465761   \n",
       "406  [personal, think, percent, ridership, system, ...          0.522549   \n",
       "407  [today, marooned, Brooklyn, train, run, want, ...          0.522266   \n",
       "\n",
       "     text_length                                           combined  \\\n",
       "397         1432  today TImes Afzal Hossain New York City subway...   \n",
       "398           20                                         fee Ugarte   \n",
       "399           70             oops think misquote quote price Ugarte   \n",
       "400          176  couple great iPhone app CityTransit map line s...   \n",
       "401          300  young lady ahead curve speak April adn MTA law...   \n",
       "402          117  Subway Service Specialists too-close-too-home ...   \n",
       "403          209  hey gon thing willing expect actually answer t...   \n",
       "404          202  lol QB like sit wooden platform bench jean lon...   \n",
       "405          115  QB awesome bet no-fun MTA shut love snappy uni...   \n",
       "406          335  personal think percent ridership system person...   \n",
       "407          104        today marooned Brooklyn train run want help   \n",
       "\n",
       "     predicted_summary_post  \n",
       "397                    True  \n",
       "398                   False  \n",
       "399                   False  \n",
       "400                   False  \n",
       "401                   False  \n",
       "402                   False  \n",
       "403                   False  \n",
       "404                   False  \n",
       "405                   False  \n",
       "406                   False  \n",
       "407                   False  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-modern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
