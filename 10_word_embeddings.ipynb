{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valid-capacity",
   "metadata": {},
   "source": [
    "# Exporing Semantic Relationship with Word Embeddings\n",
    "\n",
    "Types of Embeddings\n",
    "- word2vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Deep contextualized embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-palestine",
   "metadata": {},
   "source": [
    "### Blueprint: Using similarity queries on pretrained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-wisdom",
   "metadata": {},
   "source": [
    "**Loading a pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clear-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Store the gnesim models locally.\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "framed-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>base_dataset</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fasttext-wiki-news-subwords-300</th>\n",
       "      <td>1.005007e+09</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conceptnet-numberbatch-17-06-300</th>\n",
       "      <td>1.225498e+09</td>\n",
       "      <td>ConceptNet, word2vec, GloVe, and OpenSubtitles...</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec-ruscorpora-300</th>\n",
       "      <td>2.084274e+08</td>\n",
       "      <td>Russian National Corpus (about 250M words)</td>\n",
       "      <td>{'dimension': 300, 'window_size': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec-google-news-300</th>\n",
       "      <td>1.743564e+09</td>\n",
       "      <td>Google News (about 100 billion words)</td>\n",
       "      <td>{'dimension': 300}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove-wiki-gigaword-50</th>\n",
       "      <td>6.918254e+07</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>{'dimension': 50}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     file_size  \\\n",
       "fasttext-wiki-news-subwords-300   1.005007e+09   \n",
       "conceptnet-numberbatch-17-06-300  1.225498e+09   \n",
       "word2vec-ruscorpora-300           2.084274e+08   \n",
       "word2vec-google-news-300          1.743564e+09   \n",
       "glove-wiki-gigaword-50            6.918254e+07   \n",
       "\n",
       "                                                                       base_dataset  \\\n",
       "fasttext-wiki-news-subwords-300   Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "conceptnet-numberbatch-17-06-300  ConceptNet, word2vec, GloVe, and OpenSubtitles...   \n",
       "word2vec-ruscorpora-300                  Russian National Corpus (about 250M words)   \n",
       "word2vec-google-news-300                      Google News (about 100 billion words)   \n",
       "glove-wiki-gigaword-50             Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "\n",
       "                                                             parameters  \n",
       "fasttext-wiki-news-subwords-300                      {'dimension': 300}  \n",
       "conceptnet-numberbatch-17-06-300                     {'dimension': 300}  \n",
       "word2vec-ruscorpora-300           {'dimension': 300, 'window_size': 10}  \n",
       "word2vec-google-news-300                             {'dimension': 300}  \n",
       "glove-wiki-gigaword-50                                {'dimension': 50}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "\n",
    "info_df = pd.DataFrame.from_dict(api.info()[\"models\"], orient=\"index\")\n",
    "info_df[[\"file_size\", \"base_dataset\", \"parameters\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "united-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-generation",
   "metadata": {},
   "source": [
    "## Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "encouraging-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 50\n",
      "v_king = [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012 ]\n",
      "v_queen = [ 0.37854   1.8233   -1.2648   -0.1043    0.35829   0.60029  -0.17538\n",
      "  0.83767  -0.056798 -0.75795 ]\n",
      "similarity: 0.7839043\n"
     ]
    }
   ],
   "source": [
    "v_king = model[\"king\"]\n",
    "v_queen = model[\"queen\"]\n",
    "\n",
    "print(\"Vector size:\", model.vector_size)\n",
    "print(\"v_king =\", v_king[:10])\n",
    "print(\"v_queen =\", v_queen[:10])\n",
    "print(\"similarity:\", model.similarity(\"king\", \"queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distinguished-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.8236179351806641),\n",
       " ('queen', 0.7839043140411377),\n",
       " ('ii', 0.7746230363845825)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"king\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stupid-stable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7839043 ,  0.47800118, -0.25490996], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_lion = model[\"lion\"]\n",
    "v_nano = model[\"nanotechnology\"]\n",
    "\n",
    "model.cosine_similarities(v_king, [v_queen, v_lion, v_nano])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "angry-mileage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kingdom', 0.701315701007843),\n",
       " ('queen', 0.6152784824371338),\n",
       " ('invited', 0.6111606359481812)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"women\", \"king\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tribal-basics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('berlin', 0.9203965663909912),\n",
       " ('frankfurt', 0.8201637268066406),\n",
       " ('vienna', 0.8182448744773865)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"paris\", \"germany\"], negative=[\"france\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metallic-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.7835100293159485)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"france\", \"capital\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-playlist",
   "metadata": {},
   "source": [
    "## Blueprints for Training and Evaluating your own Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-border",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "1. Clean text from unwanted tokens (symbols, tags, etc.)\n",
    "2. Put all words into lowercase.\n",
    "3. Use lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "scenic-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_name = \"data/reddit-selfposts.db\"\n",
    "\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select subreddit, lemmas, text from posts_nlp\", con)\n",
    "con.close()\n",
    "\n",
    "df[\"lemmas\"] = df[\"lemmas\"].str.lower().str.split()  # Lower case tokens\n",
    "sents = df[\"lemmas\"]  # Our training sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-directory",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "typical-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, npmi_scorer\n",
    "\n",
    "phrases = Phrases(\n",
    "    sents, min_count=10, threshold=0.3, delimiter=\"-\", scoring=npmi_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adaptive-honduras",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|had|to|replace|the|timing-belt|in|my|mercedes-c300\n"
     ]
    }
   ],
   "source": [
    "sent = \"I had to replace the timing belt in my mercedes c300\".split()\n",
    "phrased = phrases[sent]\n",
    "print(*phrased, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "vocational-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_df = pd.DataFrame.from_dict(\n",
    "    phrases.export_phrases(), orient=\"index\", columns=[\"score\"]\n",
    ")\n",
    "phrase_df.index.name = \"phrase\"\n",
    "phrase_df = phrase_df.reset_index()\n",
    "phrase_df = (\n",
    "    phrase_df[[\"phrase\", \"score\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"score\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "phrase_df[\"phrase\"] = phrase_df[\"phrase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "placed-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>0.800502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           phrase     score\n",
       "87  mercedes-benz  0.800502"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_df[phrase_df[\"phrase\"].str.contains(\"mercedes\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-portable",
   "metadata": {},
   "source": [
    "From the result, threshold should be larger than 0.5 and smaller and 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "popular-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "    sents, min_count=10, threshold=0.7, delimiter=\"-\", scoring=npmi_scorer\n",
    ")\n",
    "df[\"phrased_lemmas\"] = df[\"lemmas\"].map(lambda s: phrases[s])\n",
    "sents = df[\"phrased_lemmas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-pregnancy",
   "metadata": {},
   "source": [
    "### Blueprint: Training Models with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cutting-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sents,  # Tokenized input sentences\n",
    "    vector_size=100,  # Size of word vectors (default 100)\n",
    "    window=2,  # Context window size (default 5)\n",
    "    sg=1,  # Use skip-gram (default 0 = CBOW)\n",
    "    negative=5,  # Number of negative samples (default 5)\n",
    "    min_count=5,  # Ignore infrequent words (default 5)\n",
    "    workers=4,  # Number of threads (default 3)\n",
    "    epochs=5,  # Number of epochs (default 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "immune-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/autos_w2v_100_2_full.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "outstanding-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dated-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec\n",
    "\n",
    "model_path = \"./models\"\n",
    "model_prefix = \"autos\"\n",
    "\n",
    "param_grid = {\n",
    "    \"w2v\": {\"variant\": [\"cbow\", \"sg\"], \"window\": [2, 5, 30]},\n",
    "    \"ft\": {\"variant\": [\"sg\"], \"window\": [5]},\n",
    "}\n",
    "size = 100\n",
    "\n",
    "for algo, params in param_grid.items():\n",
    "    for variant in params[\"variant\"]:\n",
    "        sg = 1 if variant == \"sg\" else 0\n",
    "        for window in params[\"window\"]:\n",
    "            if algo == \"w2v\":\n",
    "                model = Word2Vec(sents, vector_size=size, window=window, sg=sg)\n",
    "            else:\n",
    "                model = FastText(sents, vector_size=size, window=window, sg=sg)\n",
    "            file_name = f\"{model_path}/{model_prefix}_{algo}_{variant}_{window}\"\n",
    "            model.wv.save_word2vec_format(file_name + \".bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-london",
   "metadata": {},
   "source": [
    "### Blueprint: Evaluating Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adjacent-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "names = [\n",
    "    \"autos_w2v_cbow_2\",\n",
    "    \"autos_w2v_sg_2\",\n",
    "    \"autos_w2v_sg_5\",\n",
    "    \"autos_w2v_sg_30\",\n",
    "    \"autos_ft_sg_5\",\n",
    "]\n",
    "models = {}\n",
    "\n",
    "for name in names:\n",
    "    file_name = f\"{model_path}/{name}.bin\"\n",
    "    models[name] = KeyedVectors.load_word2vec_format(file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "focused-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models, **kwargs):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for name, model in models:\n",
    "        df[name] = [\n",
    "            f\"{word} {score:.3f}\" for word, score in model.most_similar(**kwargs)\n",
    "        ]\n",
    "    df.index = df.index + 1  # Let row index start at 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sunset-basket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autos_w2v_cbow_2</th>\n",
       "      <th>autos_w2v_sg_2</th>\n",
       "      <th>autos_w2v_sg_5</th>\n",
       "      <th>autos_w2v_sg_30</th>\n",
       "      <th>autos_ft_sg_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mercedes 0.866</td>\n",
       "      <td>mercedes 0.739</td>\n",
       "      <td>328i 0.746</td>\n",
       "      <td>328i 0.806</td>\n",
       "      <td>bmws 0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lexus 0.823</td>\n",
       "      <td>335i 0.698</td>\n",
       "      <td>335i 0.725</td>\n",
       "      <td>xdrive 0.800</td>\n",
       "      <td>bmwfs 0.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vw 0.803</td>\n",
       "      <td>mercede 0.697</td>\n",
       "      <td>benz 0.714</td>\n",
       "      <td>335i 0.776</td>\n",
       "      <td>mercedes_benz 0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mercede 0.795</td>\n",
       "      <td>porsche 0.689</td>\n",
       "      <td>mercedes 0.703</td>\n",
       "      <td>5-serie 0.763</td>\n",
       "      <td>m135i 0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>subaru 0.792</td>\n",
       "      <td>benz 0.686</td>\n",
       "      <td>mercede 0.687</td>\n",
       "      <td>bmws 0.759</td>\n",
       "      <td>merc 0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>porsche 0.787</td>\n",
       "      <td>merc 0.670</td>\n",
       "      <td>merc 0.684</td>\n",
       "      <td>535i 0.749</td>\n",
       "      <td>525i 0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>audi 0.787</td>\n",
       "      <td>e92 0.669</td>\n",
       "      <td>135i 0.679</td>\n",
       "      <td>340i 0.746</td>\n",
       "      <td>328i 0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>benz 0.776</td>\n",
       "      <td>e39 0.663</td>\n",
       "      <td>e39 0.678</td>\n",
       "      <td>f10 0.737</td>\n",
       "      <td>mercede 0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>volvo 0.771</td>\n",
       "      <td>lexus 0.663</td>\n",
       "      <td>x5 0.678</td>\n",
       "      <td>e39 0.736</td>\n",
       "      <td>mercs 0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>volkswagen 0.756</td>\n",
       "      <td>audi 0.659</td>\n",
       "      <td>e92 0.677</td>\n",
       "      <td>x-drive 0.732</td>\n",
       "      <td>mercedes-benz 0.745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    autos_w2v_cbow_2  autos_w2v_sg_2  autos_w2v_sg_5 autos_w2v_sg_30  \\\n",
       "1     mercedes 0.866  mercedes 0.739      328i 0.746      328i 0.806   \n",
       "2        lexus 0.823      335i 0.698      335i 0.725    xdrive 0.800   \n",
       "3           vw 0.803   mercede 0.697      benz 0.714      335i 0.776   \n",
       "4      mercede 0.795   porsche 0.689  mercedes 0.703   5-serie 0.763   \n",
       "5       subaru 0.792      benz 0.686   mercede 0.687      bmws 0.759   \n",
       "6      porsche 0.787      merc 0.670      merc 0.684      535i 0.749   \n",
       "7         audi 0.787       e92 0.669      135i 0.679      340i 0.746   \n",
       "8         benz 0.776       e39 0.663       e39 0.678       f10 0.737   \n",
       "9        volvo 0.771     lexus 0.663        x5 0.678       e39 0.736   \n",
       "10  volkswagen 0.756      audi 0.659       e92 0.677   x-drive 0.732   \n",
       "\n",
       "          autos_ft_sg_5  \n",
       "1            bmws 0.844  \n",
       "2           bmwfs 0.801  \n",
       "3   mercedes_benz 0.770  \n",
       "4           m135i 0.763  \n",
       "5            merc 0.761  \n",
       "6            525i 0.752  \n",
       "7            328i 0.750  \n",
       "8         mercede 0.750  \n",
       "9           mercs 0.749  \n",
       "10  mercedes-benz 0.745  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models([(n, models[n]) for n in names], positive=\"bmw\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-property",
   "metadata": {},
   "source": [
    "**Analogy reasoning on our own model**\n",
    "\n",
    "What is to \"toyota\" as \"f150\" is to \"ford\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "reverse-delight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>autos_w2v_cbow_2</th>\n",
       "      <td>f-150 0.862</td>\n",
       "      <td>camry 0.826</td>\n",
       "      <td>s80 0.799</td>\n",
       "      <td>civic-si 0.799</td>\n",
       "      <td>e320 0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autos_w2v_sg_2</th>\n",
       "      <td>camry 0.712</td>\n",
       "      <td>f-150 0.700</td>\n",
       "      <td>sr5 0.691</td>\n",
       "      <td>89 0.673</td>\n",
       "      <td>nissan-frontier 0.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autos_w2v_sg_5</th>\n",
       "      <td>tacoma 0.705</td>\n",
       "      <td>tundra 0.659</td>\n",
       "      <td>highlander 0.644</td>\n",
       "      <td>nissan-frontier 0.640</td>\n",
       "      <td>f-150 0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autos_w2v_sg_30</th>\n",
       "      <td>4runner 0.729</td>\n",
       "      <td>tacoma 0.698</td>\n",
       "      <td>tacomas 0.658</td>\n",
       "      <td>4wd 0.657</td>\n",
       "      <td>4x4 0.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autos_ft_sg_5</th>\n",
       "      <td>f150s 0.759</td>\n",
       "      <td>tacomas 0.746</td>\n",
       "      <td>toyotas 0.737</td>\n",
       "      <td>toyo 0.734</td>\n",
       "      <td>tacoma 0.731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1              2                 3  \\\n",
       "autos_w2v_cbow_2    f-150 0.862    camry 0.826         s80 0.799   \n",
       "autos_w2v_sg_2      camry 0.712    f-150 0.700         sr5 0.691   \n",
       "autos_w2v_sg_5     tacoma 0.705   tundra 0.659  highlander 0.644   \n",
       "autos_w2v_sg_30   4runner 0.729   tacoma 0.698     tacomas 0.658   \n",
       "autos_ft_sg_5       f150s 0.759  tacomas 0.746     toyotas 0.737   \n",
       "\n",
       "                                      4                      5  \n",
       "autos_w2v_cbow_2         civic-si 0.799             e320 0.795  \n",
       "autos_w2v_sg_2                 89 0.673  nissan-frontier 0.672  \n",
       "autos_w2v_sg_5    nissan-frontier 0.640            f-150 0.638  \n",
       "autos_w2v_sg_30               4wd 0.657              4x4 0.651  \n",
       "autos_ft_sg_5                toyo 0.734           tacoma 0.731  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models(\n",
    "    [(n, models[n]) for n in names],\n",
    "    positive=[\"f150\", \"toyota\"],\n",
    "    negative=[\"ford\"],\n",
    "    topn=5,\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-greensboro",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "In reality, the Toyota Tacoma is a direct competitor to the F-150 as well as the Toyota Tundra. \n",
    "The skip-gram model with the window size 5 gives the best result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-municipality",
   "metadata": {},
   "source": [
    "## Blueprint for Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-sentence",
   "metadata": {},
   "source": [
    "### Blueprint: Applying Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "successful-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn'[plot]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "unlikely-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[\"autos_w2v_sg_30\"]\n",
    "words = model.key_to_index\n",
    "wv = [model[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "joined-christmas",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [125]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m reducer \u001b[38;5;241m=\u001b[39m \u001b[43mumap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUMAP\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m reduced_wv \u001b[38;5;241m=\u001b[39m reducer\u001b[38;5;241m.\u001b[39mfit_transform(wv)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, metric=\"cosine\", n_neighbors=15, min_dist=0.1)\n",
    "reduced_wv = reducer.fit_transform(wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-university",
   "metadata": {},
   "source": [
    "### Blueprint: Using the Tensorflow Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-announcement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
